---
title: 'OTDM - Constrained Optimization - SVM'
author: "Julian Fransen, Danila Kokin"
date: "2024-11-24"
output:
  pdf_document:
    toc: true
    toc_depth: '3'
    df_print: paged
editor_options:
  chunk_output_type: inline
---

# Section 1: implement SVM in AMPL

In this first section, we generate data using the generators given, and use this as a use case for SVMs. We implement both the primal and dual in AMPL. 

## Data generation and preprocessing

We generate the data using `gensvdat`, where we use our 2 students identifier numbers for the seed for training and test, e.g. :

`./gensvmdat tes_raw.dat 100 4624        # te =  test, s = small -> tes`

`./gensvmdat trl_raw.dat 100 7438042     # tr = train, l = large -> trl`

We create 4 files in total: a small set of 100 points and a large one of 100k points. We use the same size for testing and training. After creating the raw data files, we utilize a shell script which performs processing so make sure that the data files can be loaded into AMPL. This consist of adding a header including variables `m` and `n`, removing the `*` symbol and displaying in terminal the number of misclassifications. Finally, we copy the processed data files to the `primal/` and `dual/` directories. 

### Primal SVM Problem

We aim to solve the following optimization problem:

\[
\min_{w, \gamma, s} \frac{1}{2} w^T w + \nu e^T s
\]

subject to:
\[
Y(Aw + \gamma e) + s \geq e,
\]
\[
s \geq 0,
\]

where:

- **Decision Variables**:
  \[
  (w, \gamma, s) \in \mathbb{R}^{n+1+m},
  \]
  - \( w \in \mathbb{R}^n \): Weight vector for the hyperplane.
  - \( \gamma \in \mathbb{R} \): Bias term.
  - \( s \in \mathbb{R}^m \): Slack variables for handling misclassifications.

- **Constants**:
  - \( \nu > 0 \): Regularization parameter controlling the trade-off between margin size and misclassification penalty.
  - \( A \in \mathbb{R}^{m \times n} \): Matrix where rows represent feature vectors of the data points.
  - \( Y \in \mathbb{R}^{m \times m} \): Diagonal matrix of labels, where \( Y_{ii} = y_i \), and \( y_i \in \{-1, 1\} \).
  - \( e \in \mathbb{R}^m \): Vector of ones (\( e = [1, 1, \ldots, 1]^T \)).


### Dual SVM Formulation

We formulate the dual SVM model using explicit indices instead of matrix notation.

Objective Function
\[
\max_{\lambda} \quad \sum_{i=1}^m \lambda_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \lambda_i \lambda_j y_i y_j \left( \sum_{k=1}^n A_{ik} A_{jk} \right)
\]

subject to:
1. \[
   \sum_{i=1}^m \lambda_i y_i = 0
   \]
2. \[
   0 \leq \lambda_i \leq \nu, \quad \forall i = 1, \dots, m
   \]

where:

- \( \lambda_i \): Dual variable for the \( i \)-th data point.
- \( y_i \): Label of the \( i \)-th data point (\( \pm1 \)).
- \( A_{ik} \): Feature \( k \) of the \( i \)-th data point.
- \( \nu \): Regularization parameter.

### The Separation Hyperplane in SVM

The separation hyperplane in a Support Vector Machine (SVM) is determined by \( \mathbf{w} \) (the weight vector) and \( \gamma \) (the bias term). It defines the decision boundary that separates the two classes. The equation of the hyperplane is:
\[
\sum_{j=1}^n w_j x_j + \gamma = 0
\]

where:
- \( \mathbf{w} = [w_1, w_2, \dots, w_n] \) is the weight vector,
- \( \gamma \) is the bias term,
- \( \mathbf{x} = [x_1, x_2, \dots, x_n] \) represents the coordinates of a point in \( n \)-dimensional space.


The decision rule for classification is:
\[
f(\mathbf{x}) = \text{sign} \left( \sum_{j=1}^n w_j x_j + \gamma \right)
\]

- If \( f(\mathbf{x}) > 0 \), classify as \( +1 \).
- If \( f(\mathbf{x}) < 0 \), classify as \( -1 \).

Since the separation hyperplane only depends on `w` and $\gamma$, we know that the if these are the same for the primal and dual, then the hyperplane will be the same as well. To find the the values of `w` and $\gamma$ for the dual problem, we use these formulas:
\[
w = \sum_{i=1}^m \lambda_i y_i \phi (x_i)
\]
where $\phi$ is the identity matrix, in this particular case. 
\[
\gamma = y_k - \sum_{j=1}^n w_j \cdot A_{k,j}
\]
where `k` is the index of the first support vector. We will find support vectors based on the property: $0 < \lambda_k < \nu$ for all $k \in SV$.

# Train with small dataset:

### Primal

#### Small dataset

```{bash}
cd /Users/danilakokin/Desktop/UPC/Semester3/OTDM/OTDM_Project_2/primal
/Users/danilakokin/Downloads/ampl_macos64/ampl <<EOF
option solver cplex;
model primal.mod;
data trs.dat;
let nu := 0.9;
solve;
display n, gamma, w > mparams.dat;
quit;
EOF
```
```{bash}
cd /Users/danilakokin/Desktop/UPC/Semester3/OTDM/OTDM_Project_2/primal
chmod +x fix_params.sh
./fix_params.sh
```

```{bash}
cd /Users/danilakokin/Desktop/UPC/Semester3/OTDM/OTDM_Project_2/primal
/Users/danilakokin/Downloads/ampl_macos64/ampl <<EOF
option solver cplex;
model eval.mod;
data tes.dat;
data iparams.dat;
display gamma, w;
#display results;
display accuracy, precision, recall, f1_score;
quit;
EOF
```
#### Large dataset

```{bash}
cd /Users/danilakokin/Desktop/UPC/Semester3/OTDM/OTDM_Project_2/primal
/Users/danilakokin/Downloads/ampl_macos64/ampl <<EOF
option solver cplex;
model primal.mod;
data trl.dat;

let nu := 1.0;

solve;
display n, gamma, w > mparams.dat;
quit;
EOF
```

```{bash}
cd /Users/danilakokin/Desktop/UPC/Semester3/OTDM/OTDM_Project_2/primal
chmod +x fix_params.sh
./fix_params.sh
```

```{bash}
cd /Users/danilakokin/Desktop/UPC/Semester3/OTDM/OTDM_Project_2/primal
/Users/danilakokin/Downloads/ampl_macos64/ampl <<EOF
option solver cplex;
model eval.mod;
data tel.dat;
data iparams.dat;
display gamma, w;
display accuracy, precision, recall, f1_score;
quit;
EOF
```
### Dual

#### Small dataset

```{bash}
cd /Users/danilakokin/Desktop/UPC/Semester3/OTDM/OTDM_Project_2/dual
/Users/danilakokin/Downloads/ampl_macos64/ampl <<EOF
option solver cplex;
model dual.mod;
data trs.dat;
let nu := 0.9;
solve;
param w {j in 1..n};
for {j in 1..n} { 
    let w[j] := sum {i in 1..m} lambda[i] * y[i] * A[i, j];
}
param svi := 2;
param gamma := y[svi] - sum {j in 1..n} w[j] * A[svi, j];
display w, gamma;
quit;
EOF
```

As you can see, the values for the objective function, `w*` and $\gamma$ are identical (at least up to 5 decimals) for the dual and the primal, which means they found both exactly the same optimal hyperplane. This is consistent with theory: the dual should be exactly the same as the primal, except with fewer constraints. 

## Evaluation

... plz implement here Dan

# Section 2: applying SVMs to new dataset. 


... should also be straight forward.
...
