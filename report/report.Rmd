---
title: 'OTDM - Constrained Optimization - SVM'
author: "Julian Fransen, Danila Kokin"
date: "2024-11-24"
output:
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
editor_options:
  chunk_output_type: console
---

# Section 1: implement SVM in AMPL

In this first section, we generate data using the generators given, and use this as a use case for SVMs. We implement both the primal and dual in AMPL. 

## Data generation and preprocessing

We generate the data using `gensvdat`, where we use our 2 students identifier numbers for the seed for training and test, e.g. :

`./gensvmdat tes_raw.dat 100 4624        # te =  test, s = small -> tes`

`./gensvmdat trl_raw.dat 100 7438042     # tr = train, l = large -> trl`

We create 4 files in total: a small set of 100 points and a large one of 100k points. We use the same size for testing and training. After creating the raw data files, we utilize a shell script which performs processing so make sure that the data files can be loaded into AMPL. This consist of adding a header including variables `m` and `n`, removing the `*` symbol and displaying in terminal the number of misclassifications. Finally, we copy the processed data files to the `primal/` and `dual/` directories. 

## Primal SVM Problem

We aim to solve the following optimization problem:

\[
\min_{w, \gamma, s} \frac{1}{2} w^T w + \nu e^T s
\]

subject to:
\[
Y(Aw + \gamma e) + s \geq e,
\]
\[
s \geq 0,
\]

where:

- **Decision Variables**:
  \[
  (w, \gamma, s) \in \mathbb{R}^{n+1+m},
  \]
  - \( w \in \mathbb{R}^n \): Weight vector for the hyperplane.
  - \( \gamma \in \mathbb{R} \): Bias term.
  - \( s \in \mathbb{R}^m \): Slack variables for handling misclassifications.

- **Constants**:
  - \( \nu > 0 \): Regularization parameter controlling the trade-off between margin size and misclassification penalty.
  - \( A \in \mathbb{R}^{m \times n} \): Matrix where rows represent feature vectors of the data points.
  - \( Y \in \mathbb{R}^{m \times m} \): Diagonal matrix of labels, where \( Y_{ii} = y_i \), and \( y_i \in \{-1, 1\} \).
  - \( e \in \mathbb{R}^m \): Vector of ones (\( e = [1, 1, \ldots, 1]^T \)).


## Dual SVM Formulation

We formulate the dual SVM model using explicit indices instead of matrix notation.

### Objective Function
\[
\max_{\lambda} \quad \sum_{i=1}^m \lambda_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \lambda_i \lambda_j y_i y_j \left( \sum_{k=1}^n A_{ik} A_{jk} \right)
\]

subject to:
1. \[
   \sum_{i=1}^m \lambda_i y_i = 0
   \]
2. \[
   0 \leq \lambda_i \leq \nu, \quad \forall i = 1, \dots, m
   \]

where:

- \( \lambda_i \): Dual variable for the \( i \)-th data point.
- \( y_i \): Label of the \( i \)-th data point (\( \pm1 \)).
- \( A_{ik} \): Feature \( k \) of the \( i \)-th data point.
- \( \nu \): Regularization parameter.

# Train with small dataset:

### Primal

```{bash}
cd /home/julian/uni_folder/OTDM/OTDM_p2/OTDM/primal/
/home/julian/uni_folder/OTDM/OTDM_p2/OTDM/ampl_linux-intel64/ampl <<EOF
option solver cplex;
model primal.mod;
data trs.dat;
let nu := 0.9;
solve;
display w, gamma;
quit;
EOF
```

### Dual

```{bash}
cd /home/julian/uni_folder/OTDM/OTDM_p2/OTDM/dual/
/home/julian/uni_folder/OTDM/OTDM_p2/OTDM/ampl_linux-intel64/ampl <<EOF
option solver cplex;
model dual.mod;
data trs.dat;
let nu := 0.9;
solve;
param w {j in 1..n};
param w {j in 1..n};
for {j in 1..n} { 
    let w[j] := sum {i in 1..m} lambda[i] * y[i] * A[i, j];
}
param svi := 2;
param gamma := y[svi] - sum {j in 1..n} w[j] * A[svi, j];
display w, gamma;
quit;
EOF
```

As you can see, the values for the objective function, `w*` and $\gamma$ are identical (at least up to 5 decimals) for the dual and the primal, which means they found both exactly the same optimal hyperplane. This is consistent with theory: the dual should be exactly the same as the primal, except with fewer constraints. 

## Evaluation
... plz implement here Dan

# Section 2: applying SVMs to new dataset. 


... should also be straight forward. 



...